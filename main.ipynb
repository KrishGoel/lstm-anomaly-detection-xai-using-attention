{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d02643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstmxai.utils import load_and_preprocess, raw_data, LSTMModel, load_model, plot_ecg, normalize_array\n",
    "from lstmxai.train import train_LSTM\n",
    "from lstmxai.evaluate import evaluate_LSTM\n",
    "from lstmxai.predict import predict\n",
    "from lstmxai.attention import collect_attention_weights, plot_attention_weights\n",
    "from lstmxai.saliency import compute_saliency_map, compute_all_saliency_maps\n",
    "# from lstmxai.perturbation_analysis import perturbe_data, perturbation_analysis\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e357dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./data/ecg5000.csv\"\n",
    "\n",
    "data, labels = raw_data(data_file)\n",
    "train_loader, test_loader = load_and_preprocess(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7607db25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available model files:\n",
      "1. LSTM_0.9413.pth\n",
      "LSTM Model Config: {'Input Layer Dimensions': 1, 'Hidden Layer Dimensions': 80, 'Number of Layers:': 3, 'Output Dimensions': 1, 'Model on device': device(type='cpu')}\n",
      "Model loaded from ./models/LSTM_0.9413.pth.\n"
     ]
    }
   ],
   "source": [
    "# User input to choose whether to load an existing model or train a new one\n",
    "user_input = input(\"Enter 'load' to load an existing model or 'train' to train a new one: \")\n",
    "\n",
    "if user_input == \"load\":\n",
    "    # List all .pth files in the ./models/ directory\n",
    "    model_files = [file for file in os.listdir(\"./models/\") if file.endswith(\".pth\")]\n",
    "    if len(model_files) == 0:\n",
    "        print(\"No model files found in the ./models/ directory.\")\n",
    "    else:\n",
    "        print(\"Available model files:\")\n",
    "        for i, file in enumerate(model_files):\n",
    "            print(f\"{i+1}. {file}\")\n",
    "        model_index = int(input(\"Enter the index of the model file to load: \")) - 1\n",
    "        if model_index < 0 or model_index >= len(model_files):\n",
    "            print(\"Invalid model index.\")\n",
    "        else:\n",
    "            model_path = os.path.join(\"./models/\", model_files[model_index])\n",
    "            loaded_model = load_model(model_path)\n",
    "            print(f\"Model loaded from {model_path}.\")\n",
    "\n",
    "elif user_input == \"train\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTMModel(device = device)\n",
    "    model.to(device)\n",
    "    trained_model = train_LSTM(model, train_loader, test_loader, n_epochs=150)\n",
    "    loaded_model = load_model(trained_model)\n",
    "    print(f\"Loaded the new model trained from scratch. Saved to {trained_model}.\")\n",
    "\n",
    "else:\n",
    "    print(\"Invalid input. Please enter 'load' or 'train'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(loaded_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e121c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_LSTM(loaded_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = collect_attention_weights(loaded_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ee22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the summed attention weights\n",
    "attention_sum = np.sum(attention_weights, axis=0)\n",
    "plt.plot(attention_sum)\n",
    "plt.xlabel('Index')\n",
    "plt.title('Summed Attention Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9573fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_maps = compute_all_saliency_maps(loaded_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the summed saliency maps\n",
    "saliency_sum = np.sum(saliency_maps, axis=0)\n",
    "plt.plot(saliency_sum)\n",
    "plt.xlabel('Time')\n",
    "plt.title('Summed Saliency Gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summed_saliency_map = normalize_array(np.sum(saliency_maps, axis=0))\n",
    "summed_attention_map = normalize_array(np.sum(attention_weights, axis=0))\n",
    "\n",
    "plt.plot(summed_saliency_map)\n",
    "plt.plot(summed_attention_map)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Values')\n",
    "plt.title('Summed Saliency and Attention Maps')\n",
    "plt.legend(['Saliency', 'Attention'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "summed_saliency_map = normalize_array(np.sum(saliency_maps, axis=0))\n",
    "summed_attention_map = normalize_array(np.sum(attention_weights, axis=0))\n",
    "\n",
    "# Calculate the area of intersection\n",
    "area_saliency = abs(simpson(np.minimum(summed_saliency_map, 0))) + abs(simpson(np.maximum(summed_saliency_map, 0)))\n",
    "area_attention = abs(simpson(np.minimum(summed_attention_map, 0))) + abs(simpson(np.maximum(summed_attention_map, 0)))\n",
    "area_intersection = abs(simpson(np.minimum(summed_saliency_map, summed_attention_map)))\n",
    "\n",
    "# Print the area of intersection\n",
    "print(\"Area of Saliency:\", area_saliency)\n",
    "print(\"Area of Attention:\", area_attention)\n",
    "print(\"Area of Intersection:\", area_intersection)\n",
    "print(\"Percentage Overlap:\", (area_intersection / (area_saliency + area_attention - area_intersection))*100)\n",
    "\n",
    "plt.plot(summed_saliency_map)\n",
    "plt.plot(summed_attention_map)\n",
    "# plt.plot(np.minimum(summed_saliency_map, summed_attention_map))\n",
    "plt.fill_between(np.arange(len(summed_saliency_map)), np.minimum(summed_saliency_map, summed_attention_map), alpha=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Values')\n",
    "plt.title('Summed Saliency and Attention Maps')\n",
    "plt.legend(['Saliency', 'Attention', 'Intersection'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2325d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_test_loader = perturbe_data(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
